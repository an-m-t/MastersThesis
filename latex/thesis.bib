@article{error-measures,
title = {Another look at measures of forecast accuracy},
journal = {International Journal of Forecasting},
volume = {22},
number = {4},
pages = {679-688},
year = {2006},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2006.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169207006000239},
author = {Rob J. Hyndman and Anne B. Koehler},
keywords = {Forecast accuracy, Forecast evaluation, Forecast error measures, M-competition, Mean absolute scaled error},
abstract = {We discuss and compare measures of accuracy of univariate time series forecasts. The methods used in the M-competition as well as the M3-competition, and many of the measures recommended by previous authors on this topic, are found to be degenerate in commonly occurring situations. Instead, we propose that the mean absolute scaled error become the standard measure for comparing forecast accuracy across multiple time series.}
}

@article{order-important,
  author  = {Guo Yu and Jacob Bien},
  title   = {Learning Local Dependence In Ordered Data},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {42},
  pages   = {1--60},
  url     = {http://jmlr.org/papers/v18/16-198.html}
}

@article{timeseries,
  author = {Emanuel Parzen},
  title = {{An Approach to Time Series Analysis}},
  volume = {32},
  journal = {The Annals of Mathematical Statistics},
  number = {4},
  publisher = {Institute of Mathematical Statistics},
  pages = {951 -- 989},
  year = {1961},
  doi = {10.1214/aoms/1177704840},
  URL = {https://doi.org/10.1214/aoms/1177704840}
}


@book{isomers,
  title={Chemistry for Pharmacy Students: General, Organic and Natural Product Chemistry},
  author={Sarker, P.S.D. and Nahar, L.},
  isbn={9781118687536},
  url={https://books.google.de/books?id=XAgpYnuMUToC},
  year={2013},
  publisher={Wiley}
}
@book{spatial-data,
  title={Spatial Data Analysis: Models, Methods and Techniques},
  author={Fischer, M.M. and Wang, J.},
  isbn={9783642217203},
  series={SpringerBriefs in Regional Science},
  url={https://books.google.de/books?id=Pa6q0muVIRYC},
  year={2011},
  publisher={Springer Berlin Heidelberg}
}

@article{atoms-graphs,
author = {Ali, Akbar and Das, Kinkar and Dimitrov, Darko and Furtula, Boris},
year = {2021},
month = {03},
pages = {},
title = {Atom-bond connectivity index of graphs: a review over extremal results and bounds},
volume = {5},
journal = {Discrete Mathematics Letters},
doi = {10.47443/dml.2020.0069}
}

@article{chemical-compounds,
  title={A Neural Device for Searching Direct Correlations between Structures and Properties of Chemical Compounds},
  author={Igor I. Baskin and Vladimir A. Palyulin and Nikolai S. Zefirov},
  journal={J. Chem. Inf. Comput. Sci.},
  year={1997},
  volume={37},
  pages={715-721},
  url={https://api.semanticscholar.org/CorpusID:15390639}
}

@article{PCA-tutorial,
  author       = {Jonathon Shlens},
  title        = {A Tutorial on Principal Component Analysis},
  journal      = {CoRR},
  volume       = {abs/1404.1100},
  year         = {2014},
  url          = {http://arxiv.org/abs/1404.1100},
  eprinttype    = {arXiv},
  eprint       = {1404.1100},
  timestamp    = {Mon, 13 Aug 2018 16:46:55 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/Shlens14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{PCA-optimisation,
title = {Flexible unsupervised feature extraction for image classification},
journal = {Neural Networks},
volume = {115},
pages = {65-71},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300814},
author = {Yang Liu and Feiping Nie and Quanxue Gao and Xinbo Gao and Jungong Han and Ling Shao},
keywords = {Dimensionality reduction, Unsupervised, Feature extraction},
abstract = {Dimensionality reduction is one of the fundamental and important topics in the fields of pattern recognition and machine learning. However, most existing dimensionality reduction methods aim to seek a projection matrix W such that the projection WTx is exactly equal to the true low-dimensional representation. In practice, this constraint is too rigid to well capture the geometric structure of data. To tackle this problem, we relax this constraint but use an elastic one on the projection with the aim to reveal the geometric structure of data. Based on this context, we propose an unsupervised dimensionality reduction model named flexible unsupervised feature extraction (FUFE) for image classification. Moreover, we theoretically prove that PCA and LPP, which are two of the most representative unsupervised dimensionality reduction models, are special cases of FUFE, and propose a non-iterative algorithm to solve it. Experiments on five real-world image databases show the effectiveness of the proposed model.}
}

@book{PCA-joliffe,
  title={Principal Component Analysis},
  author={Jolliffe, I.T.},
  isbn={9780387954424},
  lccn={2002019560},
  series={Springer Series in Statistics},
  url={https://books.google.de/books?id=TtVF-ao4fI8C},
  year={2002},
  publisher={Springer}
}

@article{relation-centered-uncentered-PCA,
  author          = {Cadima, Jorge and Jolliffe, Ian},
  journal         = {Pakistan Joudnal of Statistics},
  number          = {},
  title           = {ON RELATIONSHIPS BETWEEN UNCENTRED AND COLUMN-CENTRED PRINCIPAL COMPONENT ANALYSIS},
  volume          = {25(4)},
  year            = {2009}
}

@article{PCA-SVD,
author = {Dash, Prasannajit and Nayak, Maya and Das, Guruprasad},
year = {2014},
month = {05},
pages = {Page : 21-27},
title = {Principal Component Analysis using Singular Value Decomposition for Image Compression},
volume = {Volume 93},
journal = {International Journal of Computer Applications(IJCA)},
doi = {10.5120/16243-5795}
}

@book{ICA-book,
    author = {Stone, James V.},
    title = "{Independent Component Analysis: A Tutorial Introduction}",
    publisher = {The MIT Press},
    year = {2004},
    month = {09},
    abstract = "{A tutorial-style introduction to a class of methods for extracting independent signals from a mixture of signals originating from different physical sources; includes MatLab computer code examples.Independent component analysis (ICA) is becoming an increasingly important tool for analyzing large data sets. In essence, ICA separates an observed set of signal mixtures into a set of statistically independent component signals, or source signals. In so doing, this powerful method can extract the relatively small amount of useful information typically found in large data sets. The applications for ICA range from speech processing, brain imaging, and electrical brain signals to telecommunications and stock predictions.In Independent Component Analysis, Jim Stone presents the essentials of ICA and related techniques (projection pursuit and complexity pursuit) in a tutorial style, using intuitive examples described in simple geometric terms. The treatment fills the need for a basic primer on ICA that can be used by readers of varying levels of mathematical sophistication, including engineers, cognitive scientists, and neuroscientists who need to know the essentials of this evolving method. An overview establishes the strategy implicit in ICA in terms of its essentially physical underpinnings and describes how ICA is based on the key observations that different physical processes generate outputs that are statistically independent of each other. The book then describes what Stone calls "the mathematical nuts and bolts" of how ICA works. Presenting only essential mathematical proofs, Stone guides the reader through an exploration of the fundamental characteristics of ICA. Topics covered include the geometry of mixing and unmixing; methods for blind source separation; and applications of ICA, including voice mixtures, EEG, fMRI, and fetal heart monitoring. The appendixes provide a vector matrix tutorial, plus basic demonstration computer code that allows the reader to see how each mathematical method described in the text translates into working Matlab computer code.}",
    isbn = {9780262257046},
    doi = {10.7551/mitpress/3717.001.0001},
    url = {https://doi.org/10.7551/mitpress/3717.001.0001},
}

@inproceedings{Random-projection,
author = {Bingham, Ella and Mannila, Heikki},
title = {Random Projection in Dimensionality Reduction: Applications to Image and Text Data},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502546},
doi = {10.1145/502512.502546},
abstract = {Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burden-some computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {245–250},
numpages = {6},
keywords = {dimensionality reduction, image data, text document data, high-dimensional data, random projection},
location = {San Francisco, California},
series = {KDD '01}
}

@article{distance-measure,
title = {Local distances preserving based manifold learning},
journal = {Expert Systems with Applications},
volume = {139},
pages = {112860},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.112860},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419305706},
author = {Rassoul Hajizadeh and A. Aghagolzadeh and M. Ezoji},
keywords = {Dimension reduction, Manifold learning, Euclidean distance, Recognition rate, Local distance},
abstract = {In this paper a local manifold learning method based on the local distances preserving (LDP) is proposed. LDP focuses on extracting and preserving the local distances between the data points. In LDP, the coefficients between each data point and its neighbors are calculated based on the inverse of Euclidean distances between them. Then, a minimization function, which is in accordance with the calculated coefficients, is proposed for calculating the embedded data manifold in the low-dimensional representation space. Matching the way of calculation the coefficients and the proposed minimization function is the main significance of the proposed LDP method. In addition, the proposed LDP method shows less sensitivity to the initial parameters such as the number of neighbors and the value of variance. Also, a stochastic based extension of LDP (SLDP) manifold learning method is proposed. The proposed method is compared with the common manifold learning methods based on the achievable recognition rate and the power of the local distances preserving. The experiments have been done on two different kinds of databases: HODA Persian handwritten character database and ORL face image database. The results demonstrate the suitable performance of LDP and SLDP. Also, the results show the robustness of the proposed method to the number of neighbors and the value of variance parameter. Moreover, the proposed method of calculating the embedded data points in LDP and SLDP has less complexity than the similar local manifold learning methods, Laplacian eigenmaps (LEM) and stochastic LEM (SLEM).}
}

@article{weighted-dtw,
title = {Weighted dynamic time warping for time series classification},
journal = {Pattern Recognition},
volume = {44},
number = {9},
pages = {2231-2240},
year = {2011},
note = {Computer Analysis of Images and Patterns},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2010.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S003132031000484X},
author = {Young-Seon Jeong and Myong K. Jeong and Olufemi A. Omitaomu},
keywords = {Dynamic time warping, Adaptive weights, Weighted dynamic time warping, Modified logistic weight function, Time series classification, Time series clustering},
abstract = {Dynamic time warping (DTW), which finds the minimum path by providing non-linear alignments between two time series, has been widely used as a distance measure for time series classification and clustering. However, DTW does not account for the relative importance regarding the phase difference between a reference point and a testing point. This may lead to misclassification especially in applications where the shape similarity between two sequences is a major consideration for an accurate recognition. Therefore, we propose a novel distance measure, called a weighted DTW (WDTW), which is a penalty-based DTW. Our approach penalizes points with higher phase difference between a reference point and a testing point in order to prevent minimum distance distortion caused by outliers. The rationale underlying the proposed distance measure is demonstrated with some illustrative examples. A new weight function, called the modified logistic weight function (MLWF), is also proposed to systematically assign weights as a function of the phase difference between a reference point and a testing point. By applying different weights to adjacent points, the proposed algorithm can enhance the detection of similarity between two time series. We show that some popular distance measures such as DTW and Euclidean distance are special cases of our proposed WDTW measure. We extend the proposed idea to other variants of DTW such as derivative dynamic time warping (DDTW) and propose the weighted version of DDTW. We have compared the performances of our proposed procedures with other popular approaches using public data sets available through the UCR Time Series Data Mining Archive for both time series classification and clustering problems. The experimental results indicate that the proposed approaches can achieve improved accuracy for time series classification and clustering problems.}
}

@INPROCEEDINGS{dtw-image,
  author={Diab, Diab M. and AsSadhan, Basil and Binsalleeh, Hamad and Lambotharan, Sangarapillai and Kyriakopoulos, Konstantinos G. and Ghafir, Ibrahim},
  booktitle={2019 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)}, 
  title={Anomaly Detection Using Dynamic Time Warping}, 
  year={2019},
  volume={},
  number={},
  pages={193-198},
  doi={10.1109/CSE/EUC.2019.00045}}


@inproceedings{scaling-dtw,
author = {Keogh, Eamonn J. and Pazzani, Michael J.},
title = {Scaling up Dynamic Time Warping for Datamining Applications},
year = {2000},
isbn = {1581132336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/347090.347153},
doi = {10.1145/347090.347153},
booktitle = {Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {285–289},
numpages = {5},
keywords = {dynamic time warping, similarity measures, time series},
location = {Boston, Massachusetts, USA},
series = {KDD '00}
}

@inproceedings{introduction-dtw,
author = {Berndt, Donald J. and Clifford, James},
title = {Using Dynamic Time Warping to Find Patterns in Time Series},
year = {1994},
publisher = {AAAI Press},
abstract = {Knowledge discovery in databases presents many interesting challenges within the context of providing computer tools for exploring large data archives. Electronic data repositories are growing quickly and contain data from commercial, scientific, and other domains. Much of this data is inherently temporal, such as stock prices or NASA telemetry data. Detecting patterns in such data streams or time series is an important knowledge discovery task. This paper describes some preliminary experiments with a dynamic programming approach to the problem. The pattern detection algorithm is based on the dynamic time warping technique used in the speech recognition field.},
booktitle = {Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining},
pages = {359–370},
numpages = {12},
keywords = {time series, dynamic time warping, knowledge discovery, dynamic programming, pattern analysis},
location = {Seattle, WA},
series = {AAAIWS'94}
}


@misc{LLE,
title={Locally Linear Embedding and its Variants: Tutorial and Survey}, 
author={Benyamin Ghojogh and Ali Ghodsi and Fakhri Karray and Mark Crowley},
year={2020},
eprint={2011.10925},
archivePrefix={arXiv},
primaryClass={stat.ML}
}

@article{drawback-LLE,
title = {The LLE and a linear mapping},
journal = {Pattern Recognition},
volume = {39},
number = {9},
pages = {1799-1804},
year = {2006},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2006.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0031320306001312},
author = {F.C. Wu and Z.Y. Hu},
keywords = {Locally linear embedding (LLE), Linear mapping, Principal component analysis (PCA)},
abstract = {The locally linear embedding (LLE) is considered an effective algorithm for dimensionality reduction. In this short note, some of its key properties are studied. In particular, we show that: (1) there always exists a linear mapping from the high-dimensional space to the low-dimensional space such that all the constraint conditions in the LLE can be satisfied. The implication of the existence of such a linear mapping is that the LLE cannot guarantee a one-to-one mapping from the high-dimensional space to the low-dimensional space for a given data set; (2) if the LLE is required to globally preserve distance, it must be a PCA mapping; (3) for a given high-dimensional data set, there always exists a local distance-preserving LLE. The above results can bring some new insights into a better understanding of the LLE.}
}

@inproceedings{t-SNE,
author="Balamurali, Mehala",
editor="Daya Sagar, B.S.
and Cheng, Qiuming
and McKinley, Jennifer
and Agterberg, Frits",
title="t-Distributed Stochastic Neighbor Embedding",
bookTitle="Encyclopedia of Mathematical Geosciences",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="1--9",
isbn="978-3-030-26050-7",
doi="10.1007/978-3-030-26050-7_446-1",
url="https://doi.org/10.1007/978-3-030-26050-7_446-1"
}


@misc{tsne-drawback,
      title={Automatic Selection of t-SNE Perplexity}, 
      author={Yanshuai Cao and Luyu Wang},
      year={2017},
      eprint={1708.03229},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{umap-implementation,
  title={UMAP: Uniform Manifold Approximation and Projection},
  author={McInnes, Leland and Healy, John and Saul, Nathaniel and Grossberger, Lukas},
  journal={The Journal of Open Source Software},
  volume={3},
  number={29},
  pages={861},
  year={2018}
}

@misc{linear-dim-red,
      title={Linear Dimensionality Reduction: Survey, Insights, and Generalizations}, 
      author={John P. Cunningham and Zoubin Ghahramani},
      year={2016},
      eprint={1406.0873},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{lda,
  author          = {Tharwat, Alaa and Gaber, Tarek and Ibrahim, Abdelhameed and Hassanien, Aboul Ella},
  journal         = {AI Communications},
  number          = {2},
  title           = {Linear discriminant analysis: A detailed tutorial},
  volume          = {30},
  year            = {2017}
}

@article{lsda,
  author={Su, Bing and Ding, Xiaoqing and Wang, Hao and Wu, Ying},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Discriminative Dimensionality Reduction for Multi-Dimensional Sequences}, 
  year={2018},
  volume={40},
  number={1},
  pages={77-91},
  doi={10.1109/TPAMI.2017.2665545}
  }

$ Uncentered PCA
@ARTICLE{pca-not-for-discrete,
title = {SOCIOECONOMIC STATUS MEASUREMENT WITH DISCRETE PROXY VARIABLES: IS PRINCIPAL COMPONENT ANALYSIS A RELIABLE ANSWER?},
author = {Kolenikov, Stanislav and Angeles, Gustavo},
year = {2009},
journal = {Review of Income and Wealth},
volume = {55},
number = {1},
pages = {128-165},
abstract = {The last several years have seen a growth in the number of publications in economics that use principal component analysis (PCA) in the area of welfare studies. This paper explores the ways discrete data can be incorporated into PCA. The effects of discreteness of the observed variables on the PCA are reviewed. The statistical properties of the popular Filmer and Pritchett (2001) procedure are analyzed. The concepts of polychoric and polyserial correlations are introduced with appropriate references to the existing literature demonstrating their statistical properties. A large simulation study is carried out to compare various implementations of discrete data PCA. The simulation results show that the currently used method of running PCA on a set of dummy variables as proposed by Filmer and Pritchett (2001) can be improved upon by using procedures appropriate for discrete data, such as retaining the ordinal variables without breaking them into a set of dummy variables or using polychoric correlations. An empirical example using Bangladesh 2000 Demographic and Health Survey data helps in explaining the differences between procedures.},
url = {https://EconPapers.repec.org/RePEc:bla:revinw:v:55:y:2009:i:1:p:128-165}
}

@article{sign-flip,
author = {Bro, Rasmus and Acar, Evrim and Kolda, Tamara},
year = {2008},
month = {02},
pages = {135 - 140},
title = {Resolving the sign ambiguity in the singular value decomposition},
volume = {22},
journal = {Journal of Chemometrics},
doi = {10.1002/cem.1122}
}

@article{uncentered-pca-burned,
  author          = {Alexandris, Nikos and Gupta, Sandeep and Koutsias, Nikos},
  journal         = {Open Geospatial Data, Software and Standards},
  number          = {},
  title           = {Remote sensing of burned areas via PCA, Part 1; centering, scaling and EVD vs SVD},
  volume          = {},
  year            = {2017}
}

@inproceedings{pca-only-independent,
author = {Han, Fang and Liu, Han},
title = {Principal Component Analysis on Non-Gaussian Dependent Data},
year = {2013},
publisher = {JMLR.org},
abstract = {In this paper, we analyze the performance of a semiparametric principal component analysis named Copula Component Analysis (COCA) (Han \& Liu, 2012) when the data are dependent. The semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate Gaussian. We study the scenario where the observations are drawn from non-i.i.d. processes (m-dependency or a more general ϕ-mixing case). We show that COCA can allow weak dependence. In particular, we provide the generalization bounds of convergence for both support recovery and parameter estimation of COCA for the dependent data. We provide explicit sufficient conditions on the degree of dependence, under which the parametric rate can be maintained. To our knowledge, this is the first work analyzing the theoretical performance of PCA for the dependent data in high dimensional settings. Our results strictly generalize the analysis in Han \& Liu (2012) and the techniques we used have the separate interest for analyzing a variety of other multivariate statistical methods.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {I–240–I–248},
location = {Atlanta, GA, USA},
series = {ICML'13}
}


@unpublished{dropp,
  author = {Beer, Anna and Palotás, Olivér and Maldonado, Andrea and Draganov, Andrew and Assent, Ira},
  note   = {UNPUBLISHED},
  title  = {DROPP: Structure-aware PCA for Ordered Data - A General Method and its Applications in Climate Research and Molecular Dynamics},
  year   = {2023}
}

@article{global-local-structure,
author = {Wang, Yingfan and Huang, Haiyang and Rudin, Cynthia and Shaposhnik, Yaron},
title = {Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMap, and PaCMAP for Data Visualization},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Dimension reduction (DR) techniques such as t-SNE, UMAP, and TriMap have demonstrated impressive visualization performance on many real-world datasets. One tension that has always faced these methods is the trade-off between preservation of global structure and preservation of local structure: these methods can either handle one or the other, but not both. In this work, our main goal is to understand what aspects of DR methods are important for preserving both local and global structure: it is difficult to design a better method without a true understanding of the choices we make in our algorithms and their empirical impact on the low-dimensional embeddings they produce. Towards the goal of local structure preservation, we provide several useful design principles for DR loss functions based on our new understanding of the mechanisms behind successful DR methods. Towards the goal of global structure preservation, our analysis illuminates that the choice of which components to preserve is important. We leverage these insights to design a new algorithm for DR, called Pairwise Controlled Manifold Approximation Projection (PaCMAP), which preserves both local and global structure. Our work provides several unexpected insights into what design choices both to make and avoid when constructing DR algorithms.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {201},
numpages = {73},
keywords = {dimension reduction, data visualization}
}

  

@misc{website-tsne,
  author       = {scikit-learn developers},
  title        = {sklearn.manifold.TSNE},
  year         = {2023},
  note         = {Accessed September 23, 2023. \url{https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html}}
}

@misc{website-pca,
  author       = {scikit-learn developers},
  title        = {scikit-learn},
  year         = {2023},
  note         = {Accessed September 25, 2023. \url{https://github.com/scikit-learn/scikit-learn/blob/55a65a2fa/sklearn/decomposition/_pca.py#L118}}
}

@ARTICLE{neighborhood-presv-embedding,
  author={Wang, Jinping and Ran, Ruisheng and Fang, Bin},
  journal={IEEE Access}, 
  title={Global and Local Structure Network for Image Classification}, 
  year={2023},
  volume={11},
  number={},
  pages={27963-27973},
  doi={10.1109/ACCESS.2023.3258972}
  }

@article{importance-local-global-struct-preserving-1,
author = {Zhou, Honghu and Wang, Jun},
year = {2019},
month = {10},
pages = {210},
title = {Laplacian Eigenmaps Dimensionality Reduction Based on Clustering-Adjusted Similarity},
volume = {12},
journal = {Algorithms},
doi = {10.3390/a12100210}
}

@inproceedings{importance-local-global-struct-preserving-2,
author = {Chen, Jianhui and Ye, Jieping and Li, Qi},
year = {2007},
month = {07},
pages = {1-8},
title = {Integrating Global and Local Structures: A Least Squares Framework for Dimensionality Reduction},
isbn = {1-4244-1180-7},
doi = {10.1109/CVPR.2007.383040}
}

@inproceedings{global-vs-local-nonlinear-dim-reduction,
 author = {Silva, Vin and Tenenbaum, Joshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Becker and S. Thrun and K. Obermayer},
 pages = {},
 publisher = {MIT Press},
 title = {Global Versus Local Methods in Nonlinear Dimensionality Reduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/5d6646aad9bcc0be55b2c82f69750387-Paper.pdf},
 volume = {15},
 year = {2002}
}

@article{global-local-structure-method,
title = {Local nonlinear dimensionality reduction via preserving the geometric structure of data},
journal = {Pattern Recognition},
volume = {143},
pages = {109663},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109663},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003643},
author = {Xiang Wang and Junxing Zhu and Zichen Xu and Kaijun Ren and Xinwang Liu and Fengyun Wang},
keywords = {Dimensionality reduction, Embedding learning, Geometric preservation, Random walk},
abstract = {Dimensionality reduction has many applications in data visualization and machine learning. Existing methods can be classified into global ones and local ones. The global methods usually learn the linear relationship in data, while the local ones learn the manifold intrinsic geometry structure, which has a significant impact on pattern recognition. However, most of existing local methods obtain an embedding with eigenvalue or singular value decomposition, where the computational complexities are very high in a large amount of high-dimensional data. In this paper, we propose a local nonlinear dimensionality reduction method named Vec2vec, which employs a neural network with only one hidden layer to reduce the computational complexity. We first build a neighborhood similarity graph from the input matrix, and then define the context of data points with the random walk properties in the graph. Finally, we train the neural network with the context of data points to learn the embedding of the matrix. We conduct extensive experiments of data classification and clustering on nine image and text datasets to evaluate the performance of our method. Experimental results show that Vec2vec is better than several state-of-the-art dimensionality reduction methods, except that it is equivalent to UMAP on data clustering tasks in the statistical hypothesis tests, but Vec2vec needs less computational time than UMAP in high-dimensional data. Furthermore, we propose a more lightweight method named Approximate Vec2vec (AVec2vec) with little performance degradation, which employs an approximate method to build the neighborhood similarity graph. AVec2vec is still better than some state-of-the-art local dimensionality reduction methods and competitive with UMAP on data classification and clustering tasks in the statistical hypothesis tests.}
}

@ARTICLE{kNN,
  author={Cover, T. and Hart, P.},
  journal={IEEE Transactions on Information Theory}, 
  title={Nearest neighbor pattern classification}, 
  year={1967},
  volume={13},
  number={1},
  pages={21-27},
  doi={10.1109/TIT.1967.1053964}
  }
@article{kNN-application,
author = {Dhanabal, Subramaniam and SA, Chandramathi},
year = {2011},
month = {01},
pages = {},
title = {A Review of various k-Nearest Neighbor Query Processing Techniques},
volume = {3},
journal = {Int. J. Comput. Appl.}
}

@article{PCA-global-structure-preserving,
	author = {{Song, Yinglei} and {Li, Yongzhong} and {Qu, Junfeng}},
	title = {Preserving Global and Local Structures for Supervised Dimensionality Reduction},
	DOI= "10.1051/matecconf/20152806003",
	url= "https://doi.org/10.1051/matecconf/20152806003",
	journal = {MATEC Web of Conferences},
	year = 2015,
	volume = 28,
	pages = "06003",
}

@inproceedings{news-ordering,
author = {Advani, Rishi and Papotti, Paolo and Asudeh, Abolfazl},
title = {Maximizing Neutrality in News Ordering},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599425},
doi = {10.1145/3580305.3599425},
abstract = {The detection of fake news has received increasing attention over the past few years, but there are more subtle ways of deceiving one's audience. In addition to the content of news stories, their presentation can also be made misleading or biased. In this work, we study the impact of the ordering of news stories on audience perception. We introduce the problems of detecting cherry-picked news orderings and maximizing neutrality in news orderings. We prove hardness results and present several algorithms for approximately solving these problems. Furthermore, we provide extensive experimental results and present evidence of potential cherry-picking in the real world.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {11–24},
numpages = {14},
keywords = {media bias, news ordering, cherry-picking, neutrality},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{inject-bias,
author = {Hamborg, Felix and Donnay, Karsten and Gipp, Bela},
year = {2019},
month = {12},
pages = {},
title = {Automated identification of media bias in news articles: an interdisciplinary literature review},
volume = {20},
journal = {International Journal on Digital Libraries},
doi = {10.1007/s00799-018-0261-y}
}

@book{jacobi-rotation,
  author         = {Hämmerlin, Günther and Hoffmann, Karl-Heinz},
  editor         = {},
  publisher      = {Springer Berlin, Heidelberg},
  title          = {Numerische Mathematik},
  year           = {1994}
}

@INPROCEEDINGS{graphLaplacianPCA,
  author={Jiang, Bo and Ding, Chris and Luo, Bio and Tang, Jin},
  booktitle={2013 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Graph-Laplacian PCA: Closed-Form Solution and Robustness}, 
  year={2013},
  volume={},
  number={},
  pages={3492-3498},
  doi={10.1109/CVPR.2013.448}
  }

@inproceedings{LaplacianEigenmaps,
author = {Belkin, Mikhail and Niyogi, Partha},
title = {Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering},
year = {2001},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.},
booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
pages = {585–591},
numpages = {7},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'01}
}

@InProceedings{GraphPCA,
author="Saerens, Marco
and Fouss, Francois
and Yen, Luh
and Dupont, Pierre",
editor="Boulicaut, Jean-Fran{\c{c}}ois
and Esposito, Floriana
and Giannotti, Fosca
and Pedreschi, Dino",
title="The Principal Components Analysis of a Graph, and Its Relationships to Spectral Clustering",
booktitle="Machine Learning: ECML 2004",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="371--383",
abstract="This work presents a novel procedure for computing (1) distances between nodes of a weighted, undirected, graph, called the Euclidean Commute Time Distance (ECTD), and (2) a subspace projection of the nodes of the graph that preserves as much variance as possible, in terms of the ECTD -- a principal components analysis of the graph. It is based on a Markov-chain model of random walk through the graph. The model assigns transition probabilities to the links between nodes, so that a random walker can jump from node to node. A quantity, called the average commute time, computes the average time taken by a random walker for reaching node j for the first time when starting from node i, and coming back to node i. The square root of this quantity, the ECTD, is a distance measure between any two nodes, and has the nice property of decreasing when the number of paths connecting two nodes increases and when the ``length'' of any path decreases. The ECTD can be computed from the pseudoinverse of the Laplacian matrix of the graph, which is a kernel. We finally define the Principal Components Analysis (PCA) of a graph as the subspace projection that preserves as much variance as possible, in terms of the ECTD. This graph PCA has some interesting links with spectral graph theory, in particular spectral clustering.",
isbn="978-3-540-30115-8"
}

@article{tICA,
    author = {Naritomi, Yusuke and Fuchigami, Sotaro},
    title = "{Slow dynamics in protein fluctuations revealed by time-structure based independent component analysis: The case of domain motions}",
    journal = {The Journal of Chemical Physics},
    volume = {134},
    number = {6},
    pages = {065101},
    year = {2011},
    month = {02},
    abstract = "{Protein dynamics on a long time scale was investigated using all-atom molecular dynamics (MD) simulation and time-structure based independent component analysis (tICA). We selected the lysine-, arginine-, ornithine-binding protein (LAO) as a target protein and focused on its domain motions in the open state. A MD simulation of the LAO in explicit water was performed for 600 ns, in which slow and large-amplitude domain motions of the LAO were observed. After extracting domain motions by rigid-body domain analysis, the tICA was applied to the obtained rigid-body trajectory, yielding slow modes of the LAO’s domain motions in order of decreasing time scale. The slowest mode detected by the tICA represented not a closure motion described by a largest-amplitude mode determined by the principal component analysis but a twist motion with a time scale of tens of nanoseconds. The slow dynamics of the LAO were well described by only the slowest mode and were characterized by transitions between two basins. The results show that tICA is promising for describing and analyzing slow dynamics of proteins.}",
    issn = {0021-9606},
    doi = {10.1063/1.3554380},
    url = {https://doi.org/10.1063/1.3554380},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/doi/10.1063/1.3554380/14739002/065101\_1\_online.pdf},
}


@inproceedings{dimensionality-curse,
author="Verleysen, Michel
and Fran{\c{c}}ois, Damien",
editor="Cabestany, Joan
and Prieto, Alberto
and Sandoval, Francisco",
title="The Curse of Dimensionality in Data Mining and Time Series Prediction",
booktitle="Computational Intelligence and Bioinspired Systems",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="758--770",
abstract="Modern data analysis tools have to work on high-dimensional data, whose components are not independently distributed. High-dimensional spaces show surprising, counter-intuitive geometrical properties that have a large influence on the performances of data analysis tools. Among these properties, the concentration of the norm phenomenon results in the fact that Euclidean norms and Gaussian kernels, both commonly used in models, become inappropriate in high-dimensional spaces. This papers presents alternative distance measures and kernels, together with geometrical methods to decrease the dimension of the space. The methodology is applied to a typical time series prediction example.",
isbn="978-3-540-32106-4"
}


@misc{air_pollution,
  author       = {Rage, Uday Kiran},
  title        = {5+ Years of Multiple Time Series Data of Hourly PM2.5 Recordings Gathered from Various Sensors Located throughout Japan (1-1-2018 to 25-4-2023)},
  year         = {2023},
  volume       = {1},
  publisher    = {Mendeley Data},
  doi          = {10.17632/phgrnvykmr.1},
  note         = {Accessed August 9, 2023. \url{https://data.mendeley.com/datasets/phgrnvykmr/1}}
}

@misc{mts-flights,
  author       = {Bhaduri, Kanishka},
  title        = {Multivariate Time Series Search},
  year         = {2011},
  publisher    = {NASA's Open Data Portal, Dashlink},
  identifier   = {DASHLINK_449},
  note         = {Accessed August 4, 2023. \url{https://data.nasa.gov/dataset/Multivariate-Time-Series-Search/mrcc-b53g}}
}

@misc{russell2000,
  author       = {Pun, Chi Seng},
  publisher    = {Mendeley Data},
  title        = {Low- and High-Dimensional Stock Price Data},
  year         = {2017},
  doi          = {0.17632/ndxfrshm74.1},
  volume       = {1},
  version      = {3}
}


@Article{cmapss-paper1,
AUTHOR = {Trinh, Hung-Cuong and Kwon, Yung-Keun},
TITLE = {An Empirical Investigation on a Multiple Filters-Based Approach for Remaining Useful Life Prediction},
JOURNAL = {Machines},
VOLUME = {6},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {35},
URL = {https://www.mdpi.com/2075-1702/6/3/35},
ISSN = {2075-1702},
ABSTRACT = {Feature construction is critical in data-driven remaining useful life (RUL) prediction of machinery systems, and most previous studies have attempted to find a best single-filter method. However, there is no best single filter that is appropriate for all machinery systems. In this work, we devise a straightforward but efficient approach for RUL prediction by combining multiple filters and then reducing the dimension through principal component analysis. We apply multilayer perceptron and random forest methods to learn the underlying model. We compare our approach with traditional single-filtering approaches using two benchmark datasets. The former approach is significantly better than the latter in terms of a scoring function with a penalty for late prediction. In particular, we note that selecting a best single filter over the training set is not efficient because of overfitting. Taken together, we validate that our multiple filters-based approach can be a robust solution for RUL prediction of various machinery systems.},
DOI = {10.3390/machines6030035}
}


@INPROCEEDINGS{cmapss-paper2,
  author={Wang, Tianyi and Jianbo Yu and Siegel, David and Lee, Jay},
  booktitle={2008 International Conference on Prognostics and Health Management}, 
  title={A similarity-based prognostics approach for Remaining Useful Life estimation of engineered systems}, 
  year={2008},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/PHM.2008.4711421}
  }


@misc{cip-computers,
  author       = {RBG / IT of Institut für Informatik der LMU},
  howpublished = {online},
  title        = {General available compute resources at IfI},
  year         = {n.d.},
  url          = {https://www.rz.ifi.lmu.de/infos/compute_en.html},
  note         = {Accessed September 19, 2023. \url{https://www.rz.ifi.lmu.de/infos/compute_en.html} },
}

@book{graph-data,
  title={Mining Graph Data},
  author={Cook, D.J. and Holder, L.B.},
  isbn={9780470073032},
  url={https://books.google.de/books?id=bHGy0_H0g8QC},
  year={2006},
  publisher={Wiley}
}


@ARTICLE{information-loss,
  author={Kambhatla, Nandakishore and Leen, Todd K.},
  journal={Neural Computation}, 
  title={Dimension Reduction by Local Principal Component Analysis}, 
  year={1997},
  volume={9},
  number={7},
  pages={1493-1516},
  doi={10.1162/neco.1997.9.7.1493}
  }

@article{fastdtw-implementation,
author = {Salvador, Stan and Chan, Philip},
title = {Toward Accurate Dynamic Time Warping in Linear Time and Space},
year = {2007},
issue_date = {October 2007},
publisher = {IOS Press},
address = {NLD},
volume = {11},
number = {5},
issn = {1088-467X},
abstract = {Dynamic Time Warping (DTW) has a quadratic time and space complexity that limits its use to small time series. In this paper we introduce FastDTW, an approximation of DTW that has a linear time and space complexity. FastDTW uses a multilevel approach that recursively projects a solution from a coarser resolution and refines the projected solution. We prove the linear time and space complexity of FastDTW both theoretically and empirically. We also analyze the accuracy of FastDTW by comparing it to two other types of existing approximate DTW algorithms: constraints (such as Sakoe-Chiba Bands) and abstraction. Our results show a large improvement in accuracy over existing methods.},
journal = {Intell. Data Anal.},
month = {oct},
pages = {561–580},
numpages = {20},
keywords = {time series alignment, time series, Dynamic time warping, time series similarity}
}

@article{2d-often,
  author          = {Kam, Hyeong Ryeol and Lee, Sung-Ho and Park, Taejung and Kim, Chang-Hun},
  journal         = {Telecommunication Systems},
  number          = {1572-9451},
  title           = {RViz: a toolkit for real domain data visualization},
  volume          = {60},
  year            = {2015},
  doi             = {10.1007/s11235-015-0034-5},
  url             = {UR  - https://doi.org/10.1007/s11235-015-0034-5},
}
@ARTICLE{pca-often,
  author={Munoz-Romero, Sergio and Gomez-Verdejo, Vanessa and Arenas-Garcia, Jeronimo},
  journal={IEEE Computational Intelligence Magazine}, 
  title={Regularized Multivariate Analysis Framework for Interpretable High-Dimensional Variable Selection}, 
  year={2016},
  volume={11},
  number={4},
  pages={24-35},
  doi={10.1109/MCI.2016.2601701}
  }

@article{pca-clustering,
  author          = {Remesan, R. and Bray, M. and Mathew, J.},
  journal         = {Journal of Environmental Informatics},
  number          = {2},
  title           = {Application of PCA and Clustering Methods in Input Selection of Hybrid Runoff Models.},
  volume          = {31},
  year            = {2018},
  month           = {June},
}

@inproceedings{nlp-sentences,
    title = "Modeling Context Words as Regions: An Ordinal Regression Approach to Word Embedding",
    author = "Jameel, Shoaib  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K17-1014",
    doi = "10.18653/v1/K17-1014",
    pages = "123--133",
    abstract = "Vector representations of word meaning have found many applications in the field of natural language processing. Word vectors intuitively represent the average context in which a given word tends to occur, but they cannot explicitly model the diversity of these contexts. Although region representations of word meaning offer a natural alternative to word vectors, only few methods have been proposed that can effectively learn word regions. In this paper, we propose a new word embedding model which is based on SVM regression. We show that the underlying ranking interpretation of word contexts is sufficient to match, and sometimes outperform, the performance of popular methods such as Skip-gram. Furthermore, we show that by using a quadratic kernel, we can effectively learn word regions, which outperform existing unsupervised models for the task of hypernym detection.",
}